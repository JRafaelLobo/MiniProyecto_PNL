{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7513d409",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3040/2424380868.py:3: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the `langchain-ollama package and should be used instead. To use it run `pip install -U `langchain-ollama` and import as `from `langchain_ollama import OllamaEmbeddings``.\n",
      "  embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c71ffdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "CHROMA_DB_PATH = \"./chroma_db\"\n",
    "COLLECTION_NAME = \"pdf_docs\"\n",
    "\n",
    "def get_vectorstore():\n",
    "    return Chroma(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        embedding_function=embeddings,\n",
    "        persist_directory=CHROMA_DB_PATH\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17ecde7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raf/Escritorio/CosasRafa/ClasesActuales/PLN/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/tmp/ipykernel_3040/1305456209.py:3: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the `langchain-ollama package and should be used instead. To use it run `pip install -U `langchain-ollama` and import as `from `langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(model=\"llama3.1\")\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "llm = Ollama(model=\"llama3.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df557c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def build_rag_chain():\n",
    "    vectorstore = get_vectorstore()\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "    prompt = PromptTemplate.from_template(\"\"\"\n",
    "Usa el siguiente contexto para responder la pregunta.\n",
    "Si no hay suficiente información, responde claramente que no está en el documento.\n",
    "\n",
    "Contexto:\n",
    "{context}\n",
    "\n",
    "Pregunta:\n",
    "{question}\n",
    "\n",
    "Respuesta:\n",
    "\"\"\")\n",
    "\n",
    "    format_docs = lambda docs: \"\\n\\n\".join(d.page_content for d in docs)\n",
    "\n",
    "    chain = (\n",
    "        RunnableParallel(\n",
    "            context=retriever | format_docs,\n",
    "            question=RunnablePassthrough()\n",
    "        )\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    return chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88a5ddea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_chat(question):\n",
    "    chain = build_rag_chain()\n",
    "    return chain.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "300058f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3040/772077193.py:7: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the `langchain-chroma package and should be used instead. To use it run `pip install -U `langchain-chroma` and import as `from `langchain_chroma import Chroma``.\n",
      "  return Chroma(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No está explícitamente mencionada en el texto proporcionado, pero se puede inferir que la tesis del artículo es defender el procesamiento del lenguaje natural fundamentado en un modelo semántico, en lugar de utilizar un mero reconocimiento de patrones y una interpretación-acción como los sistemas de diálogo de la década de 1970.\n"
     ]
    }
   ],
   "source": [
    "respuesta = rag_chat(\"¿Cuál es la tesis de este artículo?\")\n",
    "print(respuesta)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
